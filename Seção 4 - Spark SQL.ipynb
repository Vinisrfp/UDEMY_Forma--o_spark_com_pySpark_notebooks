{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 4: Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\ApacheSpark\\\\spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# DF despachantes\n",
    "schema_of_csv_desp = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('nome', StringType(), True),\n",
    "    StructField('status', StringType(), True),\n",
    "    StructField('cidade', StringType(), True),\n",
    "    StructField('vendas', StringType(), True),\n",
    "    StructField('data', DateType(), True)\n",
    "])\n",
    "despachantes = spark.read.csv(\n",
    "    r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\despachantes.csv', \n",
    "    header=False,\n",
    "    schema=schema_of_csv_desp\n",
    ")\n",
    "despachantes.createOrReplaceTempView('despachantes')\n",
    "\n",
    "# DF reclamacoes\n",
    "schema_of_csv_rec = StructType([\n",
    "    StructField('idrec', IntegerType(), True),\n",
    "    StructField('datarec', StringType(), True ),\n",
    "    StructField('iddesp', IntegerType(), True)\n",
    "])\n",
    "reclamacoes = spark.read.csv(\n",
    "    r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\reclamacoes.csv',\n",
    "    header=False,\n",
    "    schema=schema_of_csv_rec\n",
    ")\n",
    "despachantes.createOrReplaceTempView('reclamacoes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View temporaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "despachantes.createOrReplaceTempView('despachantes_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from despachantes_v1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "despachantes.createOrReplaceGlobalTempView('despachantes_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# em toda consulta de view global temporario será necessario indicar que se trata de um view global,\n",
    "# dessa forma: global_temp.<view_name>\n",
    "spark.sql('select * from global_temp.despachantes_v2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possivel criar as view com uma clausula SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '['.(line 1, pos 64)\n\n== SQL ==\nCREATE OR REPLACE TEMP VIEW desp_view AS select * from DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string]\n----------------------------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE OR REPLACE TEMP VIEW desp_view AS select * from \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdespachantes\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\viser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\viser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\viser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '['.(line 1, pos 64)\n\n== SQL ==\nCREATE OR REPLACE TEMP VIEW desp_view AS select * from DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string]\n----------------------------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMP VIEW desp_view AS select * from despachantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possivel criar as view global com uma clausula SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE OR REPLACE GLOBAL TEMP VIEW desp_view_v2 AS select * from despachantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from global_temp.desp_view_v2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 34. Comparando DataFrames com Tabelas SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|       cidade|sum(vendas)|\n",
      "+-------------+-----------+\n",
      "| Porto Alegre|      223.0|\n",
      "|  Santa Maria|       45.0|\n",
      "|Novo Hamburgo|       34.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# despachantes.createOrReplaceTempView('despachantes')\n",
    "\n",
    "# spark.sql(\"select * from despachantes\").show()\n",
    "# spark.sql(\"select nome, vendas from despachantes\").show()\n",
    "# spark.sql('select nome, vendas from despachantes where vendas > 20').show()\n",
    "spark.sql('select cidade, sum(vendas) from despachantes group by cidade order by 2 desc').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|       cidade|sum(vendas)|\n",
      "+-------------+-----------+\n",
      "| Porto Alegre|      223.0|\n",
      "|  Santa Maria|       45.0|\n",
      "|Novo Hamburgo|       34.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# despachantes.show()\n",
    "# despachantes.select('nome', 'vendas').show()\n",
    "# despachantes.select('nome', 'vendas').where(F.col(\"vendas\")>20).show() \n",
    "despachantes.groupBy('cidade').agg(sum('vendas')).orderBy(F.col(\"sum(vendas)\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aula explicativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36. Joins com Dataframes e SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOINS SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-------------------+\n",
      "|idrec|   datarec|iddesp|               nome|\n",
      "+-----+----------+------+-------------------+\n",
      "| NULL|      NULL|  NULL|   Carminda Pestana|\n",
      "|    2|2020-09-11|     2|    Deolinda Vilela|\n",
      "|    1|2020-09-12|     2|    Deolinda Vilela|\n",
      "| NULL|      NULL|  NULL|   Emídio Dornelles|\n",
      "|    3|2020-10-05|     4|Felisbela Dornelles|\n",
      "|    6|2020-01-09|     5|     Graça Ornellas|\n",
      "|    5|2020-12-06|     5|     Graça Ornellas|\n",
      "|    4|2020-10-02|     5|     Graça Ornellas|\n",
      "| NULL|      NULL|  NULL|   Matilde Rebouças|\n",
      "| NULL|      NULL|  NULL|    Noêmia   Orriça|\n",
      "| NULL|      NULL|  NULL|      Roque Vásquez|\n",
      "|    7|2020-01-05|     9|      Uriel Queiroz|\n",
      "| NULL|      NULL|  NULL|   Viviana Sequeira|\n",
      "+-----+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INNER JOIN\n",
    "spark.sql('''\n",
    "        SELECT \n",
    "            b.*\n",
    "            , a.nome \n",
    "        FROM despachantes  A\n",
    "        INNER JOIN reclamacoes b\n",
    "            ON A.id = B.iddesp\n",
    "''').show()\n",
    "\n",
    "# RIGHT JOIN\n",
    "spark.sql('''\n",
    "        SELECT \n",
    "            b.*\n",
    "            , a.nome \n",
    "        FROM despachantes  A\n",
    "        RIGHT JOIN reclamacoes b\n",
    "            ON A.id = B.iddesp\n",
    "''').show()\n",
    "\n",
    "# LEFT JOIN\n",
    "spark.sql('''\n",
    "        SELECT \n",
    "            b.*\n",
    "            , a.nome \n",
    "        FROM despachantes  A\n",
    "        LEFT JOIN reclamacoes b\n",
    "            ON A.id = B.iddesp\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-------------------+\n",
      "|idrec|   datarec|iddesp|               nome|\n",
      "+-----+----------+------+-------------------+\n",
      "|    2|2020-09-11|     2|    Deolinda Vilela|\n",
      "|    1|2020-09-12|     2|    Deolinda Vilela|\n",
      "|    3|2020-10-05|     4|Felisbela Dornelles|\n",
      "|    6|2020-01-09|     5|     Graça Ornellas|\n",
      "|    5|2020-12-06|     5|     Graça Ornellas|\n",
      "|    4|2020-10-02|     5|     Graça Ornellas|\n",
      "|    7|2020-01-05|     9|      Uriel Queiroz|\n",
      "+-----+----------+------+-------------------+\n",
      "\n",
      "+-----+----------+------+-------------------+\n",
      "|idrec|   datarec|iddesp|               nome|\n",
      "+-----+----------+------+-------------------+\n",
      "|    1|2020-09-12|     2|    Deolinda Vilela|\n",
      "|    2|2020-09-11|     2|    Deolinda Vilela|\n",
      "|    3|2020-10-05|     4|Felisbela Dornelles|\n",
      "|    4|2020-10-02|     5|     Graça Ornellas|\n",
      "|    5|2020-12-06|     5|     Graça Ornellas|\n",
      "|    6|2020-01-09|     5|     Graça Ornellas|\n",
      "|    7|2020-01-05|     9|      Uriel Queiroz|\n",
      "+-----+----------+------+-------------------+\n",
      "\n",
      "+-----+----------+------+-------------------+\n",
      "|idrec|   datarec|iddesp|               nome|\n",
      "+-----+----------+------+-------------------+\n",
      "| NULL|      NULL|  NULL|   Carminda Pestana|\n",
      "|    2|2020-09-11|     2|    Deolinda Vilela|\n",
      "|    1|2020-09-12|     2|    Deolinda Vilela|\n",
      "| NULL|      NULL|  NULL|   Emídio Dornelles|\n",
      "|    3|2020-10-05|     4|Felisbela Dornelles|\n",
      "|    6|2020-01-09|     5|     Graça Ornellas|\n",
      "|    5|2020-12-06|     5|     Graça Ornellas|\n",
      "|    4|2020-10-02|     5|     Graça Ornellas|\n",
      "| NULL|      NULL|  NULL|   Matilde Rebouças|\n",
      "| NULL|      NULL|  NULL|    Noêmia   Orriça|\n",
      "| NULL|      NULL|  NULL|      Roque Vásquez|\n",
      "|    7|2020-01-05|     9|      Uriel Queiroz|\n",
      "| NULL|      NULL|  NULL|   Viviana Sequeira|\n",
      "+-----+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INNER JOIN\n",
    "despachantes.join(\n",
    "    reclamacoes, despachantes.id == reclamacoes.iddesp, 'inner'\n",
    "        ).select('idrec','datarec','iddesp','nome'\n",
    "            ).show()\n",
    "\n",
    "# RIGHT JOIN\n",
    "despachantes.join(\n",
    "    reclamacoes, despachantes.id == reclamacoes.iddesp, 'right'\n",
    "        ).select('idrec','datarec','iddesp','nome'\n",
    "            ).show()\n",
    "\n",
    "# LEFT JOIN\n",
    "despachantes.join(\n",
    "    reclamacoes, despachantes.id == reclamacoes.iddesp, 'left'\n",
    "        ).select('idrec','datarec','iddesp','nome'\n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37. Utilizando Spark-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INICIAR SHELL: spark-sql\n",
    "\n",
    "# Ao utilizar o shell do spark-sql, você irá utilizar o SQL puro, ou seja, todos os comando vão rodar \n",
    "# 100% em SQL, isso permite rodar qualquer comando valido da lingaugem. Exemplo:\n",
    "\n",
    "show databases;\n",
    "use <database_name>;\n",
    "show tables;\n",
    "\n",
    "select * from <table>;\n",
    "\n",
    "select * from <table> where <filtro>;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38. Atividades: Faça você mesmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crie um banco de dados no DW do Spark chamado VendasVarejo, e persista todas as tabelas neste banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes = spark.read.parquet(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\Atividades\\Clientes.parquet')\n",
    "ItensVendas = spark.read.parquet(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\Atividades\\ItensVendas.parquet')\n",
    "Produtos = spark.read.parquet(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\Atividades\\Produtos.parquet')\n",
    "Vendas = spark.read.parquet(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\Atividades\\Vendas.parquet')\n",
    "Vendedores = spark.read.parquet(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\Atividades\\Vendedores.parquet')\n",
    "\n",
    "# spark.sql('CREATE DATABASE IF NOT EXISTS VendasVarejo')\n",
    "# spark.sql('USE VendasVarejo')\n",
    "# clientes.write.saveAsTable('clientes')\n",
    "# clientes.write.saveAsTable('ItensVendas')\n",
    "# clientes.write.saveAsTable('Produtos')\n",
    "# clientes.write.saveAsTable('Vendas')\n",
    "# clientes.write.saveAsTable('Vendedores')\n",
    "\n",
    "clientes.createOrReplaceTempView('clientes')\n",
    "ItensVendas.createOrReplaceTempView('itensVendas')\n",
    "Produtos.createOrReplaceTempView('produtos')\n",
    "Vendas.createOrReplaceTempView('vendas')\n",
    "Vendedores.createOrReplaceTempView('vendedores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Crie uma consulta que mostre cada item vendido: Nome do Cliente, Data da Venda, Produto, Vendedor\n",
    "e Valor total do item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+--------------------+----------------+----------+\n",
      "|          Cliente|    Data|             Produto|        Vendedor|ValorTotal|\n",
      "+-----------------+--------+--------------------+----------------+----------+\n",
      "|   Cosme Zambujal|1/1/2019|Bicicleta Altools...|    Armando Lago|   7820.85|\n",
      "|   Cosme Zambujal|1/1/2019|Bermuda Predactor...|    Armando Lago|     97.75|\n",
      "|   Cosme Zambujal|1/1/2019|Camiseta Predacto...|    Armando Lago|     135.0|\n",
      "|Gertrudes Hidalgo|1/1/2020|Luva De Ciclismo ...|   Iberê Lacerda|     150.4|\n",
      "| Antão Corte-Real|2/1/2020|Capacete Gometws ...|Jéssica Castelão|     155.0|\n",
      "| Antão Corte-Real|2/1/2020|Bicicleta Gometws...|Jéssica Castelão|    5932.0|\n",
      "| Antão Corte-Real|2/1/2019|Bicicleta Altools...|  Hélio Liberato|   7820.85|\n",
      "| Antão Corte-Real|2/1/2019|Bermuda Predactor...|  Hélio Liberato|     97.75|\n",
      "| Antão Corte-Real|2/1/2019|Bicicleta Gometws...|  Hélio Liberato|    5910.0|\n",
      "| Antão Corte-Real|3/1/2018|Bicicleta Gometws...|  Hélio Liberato|    2955.0|\n",
      "| Antão Corte-Real|3/1/2018|Bicicleta Trinc C...|  Hélio Liberato|    7658.0|\n",
      "| Antão Corte-Real|3/1/2018|Bicicleta Aro 29 ...|  Hélio Liberato|    8852.0|\n",
      "| Antão Corte-Real|3/1/2018|Camiseta Predacto...|  Hélio Liberato|     121.5|\n",
      "| Antão Corte-Real|3/1/2018|Bicicleta Gts Adv...|  Hélio Liberato|   6510.16|\n",
      "| Antão Corte-Real|4/1/2020|Bicicleta Altools...|  Hélio Liberato|   18402.0|\n",
      "| Antão Corte-Real|6/1/2019|Bicicleta Aro 29 ...|  Hélio Liberato|    7524.2|\n",
      "|Gertrudes Infante|6/1/2019|Luva De Ciclismo ...|  Hélio Liberato|     376.0|\n",
      "|Gertrudes Infante|6/1/2019|Bicicleta Gts Adv...|  Hélio Liberato|   3616.75|\n",
      "|Gertrudes Infante|6/1/2019|Camiseta Predacto...|  Hélio Liberato|     108.0|\n",
      "|Gertrudes Infante|6/1/2019|Bermuda Predactor...|  Hélio Liberato|     115.0|\n",
      "+-----------------+--------+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        clientes.Cliente\n",
    "        ,vendas.Data\n",
    "        ,produtos.Produto\n",
    "        ,vendedores.Vendedor\n",
    "        ,itensVendas.ValorTotal\n",
    "    FROM vendas \n",
    "        INNER JOIN vendedores ON vendas.VendedorId = vendedores.VendedorId\n",
    "            INNER JOIN clientes ON vendas.ClienteID = clientes.ClienteID\n",
    "                INNER JOIN itensVendas ON vendas.VendasID = itensVendas.VendasID\n",
    "                    INNER JOIN produtos ON itensVendas.ProdutoID = produtos.ProdutoID\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39. Resolução atividade 38.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. Resolução atividade 38.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. Conectando a Outras Fontes de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora vimos como exportar dados de diversos tipos de arquivos, que são bastante comum no DataLake.\n",
    "Porém, as vezes é necessário extrair de outras fontes, fontes no qual os dados não estão em formato de\n",
    "arquivos. Nesses casos é necessário se conectar ao \"Gerenciador de dados\" e realizar extração.\n",
    "\n",
    "Com spark você pode se conectar com outras fontes de dados. Você irá receber esses dados no formato de\n",
    "um dataframe, e nesse momento, você pode exportar esses dados para arquivos como: **parquet, csv, json, etc**. Poderá persistir eles como uma tabela, mas também é possivel gravar de volta os dados na mesma fonte ou até em outra fonte.\n",
    "\n",
    "**É importante ressaltar que os principios para se conectar e extrair dados são os mesmos, o único ponto importante é que você pode fazer isso atraves de um Drive JBDC ou com o pacote nativo do python pro spark.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapas:\n",
    "\n",
    "- Instalar\n",
    "- Criar banco de dados Vendas\n",
    "- Configurar acesso\n",
    "- Baixar Driver JDBC\n",
    "- Usar o Pyspark para ler, transformar e gravar os dados no Postgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. Instalando PostGreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dentro do SHELL DA MÁQUINA VIRTUAL UBUNTU LTS:**\n",
    "\n",
    "1 - atualizar o local\n",
    "    sudo apt-get update\n",
    "\n",
    "2 -instalar postgreSQL\n",
    "    sudo apt-get update && sudo apt-get install postgre-12\n",
    "\n",
    "3 - realizando login na ferramenta do postgre\n",
    "    sudo -u postgres psql\n",
    "\n",
    "4 - criando banco de dados vendas\n",
    "    create database vendas;\n",
    "\n",
    "5 - mudar para o diretorio do banco\n",
    "    c\\ vendas;\n",
    "\n",
    "6 - criar as tabelas com os scripts disponibilizados na aula\n",
    "    i\\ <caminho_do_arquivo>\n",
    "\n",
    "7 - verificar se as tabelas foram criadas\n",
    "    \\dt\n",
    "\n",
    "8 - definir senha para o usuario utilizado\n",
    "    \\password\n",
    "\n",
    "A instalação realizada já vem com o modo de autenticação que permite fazer uma autenticação com usuario e senha. Porém, existem versões do postgre que não permitem isso por padrão, então vamos ver como deveriamos fazer nesse caso:\n",
    "\n",
    "**Editar arquivo de configuração diretório inicial do sheel**\n",
    "\n",
    "1- Abrir o arquivo de configuração\n",
    "**sudo <editor_de_texto> /etc/postgresql/<versão_do_postgre>/main/pg_hba.conf**\n",
    "sudo gedit /etc/postgresql/12/main/pg_hba.conf\n",
    "\n",
    "2 - Procurar linha com autenticação md5\n",
    "Caso essas linhas estejam comentadas, deve-se descomentar:\n",
    "    host all all 127.0.0.1/32 md5\n",
    "    host all all ::1/128 md5\n",
    "\n",
    "Caso elas estejam da forma abaixo, basta mudar o \"identy\" para \"md5\" e reiniciar o postgre.\n",
    "    host all all 127.0.0.1/32 identy\n",
    "    host all all 127.0.0.1/32 identy\n",
    "\n",
    "\n",
    "**Maquina pessoal**\n",
    "\n",
    "Realizar o download do instalador no navegador web e rodar as queries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. Drive JDBC para PostGre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrir o navegador web e pesquisar pelo drive \"Driver JDBC postgre\" e realizar o downloadas do arquivo jar.\n",
    "\n",
    "O próximo passo é inicializar o pypsark apontando para o driver baixado para postgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dentro da maquina ubuntu:**\n",
    "\n",
    "pyspark --jars <caminho_do_arquivo>\n",
    "\n",
    "**No caso do visual code o arquivo devera ser passado na criação da spark session:**\n",
    "\n",
    ".config(\"spark.jars\", '<caminho_arquivo>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. Lendo e Gravando Dados no PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando dataframe com conexão que irá ler a tabela de vendas no banco de dados vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:postgresql://localhost:5432/vendas\")\\\n",
    "    .option(\"dbtable\", \"vendas\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"123456\")\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").load()\n",
    "\n",
    "resumo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando dados do dataframe no banco de dados postgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão permanece a mesma, alterando apenas alguns options: \n",
    "# Nome da tabela onde será salvo: .option(\"dbtable\", \"<nome_tabela>\")\\\n",
    "# Modo do dataframe, no caso save() .option(\"driver\", \"org.postgresql.Driver\").<modo>()\n",
    "\n",
    "# Criando dataframe selecionando dados necessários.\n",
    "venda_data =  resumo.select('data', 'total')\n",
    "\n",
    "venda_data.write\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:postgresql://localhost:5432/vendas\")\\\n",
    "    .option(\"dbtable\", \"venda_data\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"123456\")\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. MongoBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. Instalando MongoBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando instação do mongodb.\n",
    "Utilizei o mongo cloud free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. Lendo e gravando dados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymongo) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo[srv] in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymongo[srv]) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pymongo 4.10.1 does not provide the extra 'srv'\n",
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo[srv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo[srv] in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\viser\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymongo[srv]) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pymongo 4.10.1 does not provide the extra 'srv'\n",
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo[srv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().set(\"spark.jars.packages\",\n",
    "                               \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.\").setMaster(\"local\").setAppName(\"My App\").setAll([(\"spark.driver.memory\", \"40g\"), (\"spark.executor.memory\", \"50g\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\ApacheSpark\\spark-3.5.1-bin-hadoop3\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master('local[*]')\\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb+srv://vinisrfp:1XTznESq5xiVYLL2@cluster0.rpumd.mongodb.net/sample_airbnb.listingsAndReviews\")\\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb+srv://vinisrfp:1XTznESq5xiVYLL2@cluster0.rpumd.mongodb.net/sample_airbnb.listingsAndReviews\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "\n",
    "# Instalar driver odbc para conectar ao postgre\n",
    "# .config(\"spark.jars\", r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\postgresql-42.7.3.jar')\\\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados da collection MongoDB em um DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. Aplicação 1: Escrevendo no console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação do aplicativo será criada em um arquivo .py, ele foi executado no console da máquina virtual, utilizando o comando:\n",
    "\n",
    "spark-submit <caminho_arquivo>\n",
    "spark-submit aplicativo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('exemplo').getOrCreate()\n",
    "\n",
    "    arqschema = 'id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING'\n",
    "    despachantes = spark.read.csv(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\despachantes.csv', header = False, schema = arqschema)\n",
    "\n",
    "    calculo = despachantes.select('data').groupBy(year('data')).count()\n",
    "\n",
    "    calculo.write.format('console').save()\n",
    "\n",
    "    spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. Aplicação 2: Escrevendo no console com Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação do aplicativo será criada em um arquivo .py, ele foi executado no console da máquina virtual, utilizando o comando:\n",
    "\n",
    "spark-submit <caminho_aplicativo> <caminho_arquivo>\n",
    "spark-submit aplicativo.py C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\despachantes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('exemplo').getOrCreate()\n",
    "\n",
    "    arqschema = 'id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING'\n",
    "    despachantes = spark.read.csv(sys.argv[1], header = False, schema = arqschema)\n",
    "\n",
    "    calculo = despachantes.select('data').groupBy(year('data')).count()\n",
    "\n",
    "    calculo.write.format('console').save()\n",
    "\n",
    "    spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. Opção e argumentos em Linha de Comando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Argumentos**\n",
    "\n",
    "- O primeiro é sempre o nome do aplicativo\n",
    "- Podemos definir opções e valores\n",
    "- Podemos ler as opções e respectivos valores, por exemplo:\n",
    "- **programa -t [formato] -i [csv de entrada] -o [diretório de saída]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. Aplicação 3: Conversor de Formatos e Arquivos em Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
