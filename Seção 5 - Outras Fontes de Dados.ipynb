{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5: Outras Fontes de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession, lit\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\ApacheSpark\\\\spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar driver odbc para conectar ao postgre\n",
    " - .config(\"spark.jars\", r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\postgresql-42.7.3.jar')\\\n",
    "\n",
    "Instala driver conector mongo\n",
    " - spark = SparkSession.builder.master('local[*]').config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.0.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. Conectando a Outras Fontes de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora vimos como exportar dados de diversos tipos de arquivos, que são bastante comum no DataLake.\n",
    "Porém, as vezes é necessário extrair de outras fontes, fontes no qual os dados não estão em formato de\n",
    "arquivos. Nesses casos é necessário se conectar ao \"Gerenciador de dados\" e realizar extração.\n",
    "\n",
    "Com spark você pode se conectar com outras fontes de dados. Você irá receber esses dados no formato de\n",
    "um dataframe, e nesse momento, você pode exportar esses dados para arquivos como: **parquet, csv, json, etc**. Poderá persistir eles como uma tabela, mas também é possivel gravar de volta os dados na mesma fonte ou até em outra fonte.\n",
    "\n",
    "**É importante ressaltar que os principios para se conectar e extrair dados são os mesmos, o único ponto importante é que você pode fazer isso atraves de um Drive JBDC ou com o pacote nativo do python pro spark.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapas:\n",
    "\n",
    "- Instalar\n",
    "- Criar banco de dados Vendas\n",
    "- Configurar acesso\n",
    "- Baixar Driver JDBC\n",
    "- Usar o Pyspark para ler, transformar e gravar os dados no Postgre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. Instalando PostGreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dentro do SHELL DA MÁQUINA VIRTUAL UBUNTU LTS:**\n",
    "\n",
    "1 - atualizar o local\n",
    "    sudo apt-get update\n",
    "\n",
    "2 -instalar postgreSQL\n",
    "    sudo apt-get update && sudo apt-get install postgre-12\n",
    "\n",
    "3 - realizando login na ferramenta do postgre\n",
    "    sudo -u postgres psql\n",
    "\n",
    "4 - criando banco de dados vendas\n",
    "    create database vendas;\n",
    "\n",
    "5 - mudar para o diretorio do banco\n",
    "    c\\ vendas;\n",
    "\n",
    "6 - criar as tabelas com os scripts disponibilizados na aula\n",
    "    i\\ <caminho_do_arquivo>\n",
    "\n",
    "7 - verificar se as tabelas foram criadas\n",
    "    \\dt\n",
    "\n",
    "8 - definir senha para o usuario utilizado\n",
    "    \\password\n",
    "\n",
    "A instalação realizada já vem com o modo de autenticação que permite fazer uma autenticação com usuario e senha. Porém, existem versões do postgre que não permitem isso por padrão, então vamos ver como deveriamos fazer nesse caso:\n",
    "\n",
    "**Editar arquivo de configuração diretório inicial do sheel**\n",
    "\n",
    "1- Abrir o arquivo de configuração\n",
    "**sudo <editor_de_texto> /etc/postgresql/<versão_do_postgre>/main/pg_hba.conf**\n",
    "sudo gedit /etc/postgresql/12/main/pg_hba.conf\n",
    "\n",
    "2 - Procurar linha com autenticação md5\n",
    "Caso essas linhas estejam comentadas, deve-se descomentar:\n",
    "    host all all 127.0.0.1/32 md5\n",
    "    host all all ::1/128 md5\n",
    "\n",
    "Caso elas estejam da forma abaixo, basta mudar o \"identy\" para \"md5\" e reiniciar o postgre.\n",
    "    host all all 127.0.0.1/32 identy\n",
    "    host all all 127.0.0.1/32 identy\n",
    "\n",
    "\n",
    "**Maquina pessoal**\n",
    "\n",
    "Realizar o download do instalador no navegador web e rodar as queries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. Drive JDBC para PostGre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrir o navegador web e pesquisar pelo drive \"Driver JDBC postgre\" e realizar o downloadas do arquivo jar.\n",
    "\n",
    "O próximo passo é inicializar o pypsark apontando para o driver baixado para postgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dentro da maquina ubuntu:**\n",
    "\n",
    "pyspark --jars <caminho_do_arquivo>\n",
    "\n",
    "**No caso do visual code o arquivo devera ser passado na criação da spark session:**\n",
    "\n",
    ".config(\"spark.jars\", '<caminho_arquivo>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. Lendo e Gravando Dados no PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando dataframe com conexão que irá ler a tabela de vendas no banco de dados vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:postgresql://localhost:5432/vendas\")\\\n",
    "    .option(\"dbtable\", \"vendas\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"123456\")\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").load()\n",
    "\n",
    "resumo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando dados do dataframe no banco de dados postgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão permanece a mesma, alterando apenas alguns options: \n",
    "# Nome da tabela onde será salvo: .option(\"dbtable\", \"<nome_tabela>\")\\\n",
    "# Modo do dataframe, no caso save() .option(\"driver\", \"org.postgresql.Driver\").<modo>()\n",
    "\n",
    "# Criando dataframe selecionando dados necessários.\n",
    "venda_data =  resumo.select('data', 'total')\n",
    "\n",
    "venda_data.write\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:postgresql://localhost:5432/vendas\")\\\n",
    "    .option(\"dbtable\", \"venda_data\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"123456\")\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. MongoBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. Instalando MongoBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando instação do mongodb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. Lendo e gravando dados no MongoDB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
