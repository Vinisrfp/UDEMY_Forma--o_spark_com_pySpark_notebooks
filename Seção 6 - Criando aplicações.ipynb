{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 6: Criando aplicações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\ApacheSpark\\\\spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. Aplicação 1: Escrevendo no console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação do aplicativo será criada em um arquivo .py, ele foi executado no console da máquina virtual, utilizando o comando:\n",
    "\n",
    "spark-submit <caminho_arquivo>\n",
    "spark-submit aplicativo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('exemplo').getOrCreate()\n",
    "\n",
    "    arqschema = 'id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING'\n",
    "    despachantes = spark.read.csv(r'C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\despachantes.csv', header = False, schema = arqschema)\n",
    "\n",
    "    calculo = despachantes.select('data').groupBy(year('data')).count()\n",
    "\n",
    "    calculo.write.format('console').save()\n",
    "\n",
    "    spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. Aplicação 2: Escrevendo no console com Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação do aplicativo será criada em um arquivo .py, ele foi executado no console da máquina virtual, utilizando o comando:\n",
    "\n",
    "spark-submit <caminho_aplicativo> <caminho_arquivo>\n",
    "spark-submit aplicativo.py C:\\Users\\viser\\OneDrive\\Documentos\\Cursos\\notebooks\\students_pyspark\\Material disponibilizado\\download\\despachantes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('exemplo').getOrCreate()\n",
    "\n",
    "    arqschema = 'id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING'\n",
    "    despachantes = spark.read.csv(sys.argv[1], header = False, schema = arqschema)\n",
    "\n",
    "    calculo = despachantes.select('data').groupBy(year('data')).count()\n",
    "\n",
    "    calculo.write.format('console').save()\n",
    "\n",
    "    spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. Opção e argumentos em Linha de Comando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Argumentos**\n",
    "\n",
    "- O primeiro é sempre o nome do aplicativo\n",
    "- Podemos definir opções e valores\n",
    "- Podemos ler as opções e respectivos valores, por exemplo:\n",
    "- **programa -t [formato] -i [csv de entrada] -o [diretório de saída]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. Aplicação 3: Conversor de Formatos e Arquivos em Spark"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
